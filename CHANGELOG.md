# Cambios Implementados - Sistema de Scraping Optimizado

##  Objetivo
Optimizar el uso de minutos de GitHub Actions dividiendo el scraping en dos modos: diario (r谩pido) y semanal (completo), con recuperaci贸n inteligente de 404s.

##  Cambios Realizados

### 1. Base de Datos
**Archivo:** `scraper.js` (funci贸n `initDatabase`)

**Columnas agregadas a tabla `products`:**
- `status` (TEXT DEFAULT 'active'): Estado del producto ('active' o '404')
- `last_check` (DATETIME): ltima verificaci贸n del producto

### 2. Script de Migraci贸n
**Archivo nuevo:** `migrate_db.js`

**Funcionalidad:**
- Agrega columnas `status` y `last_check` a productos existentes
- Marca productos con status='404' bas谩ndose en `scraping_failures`
- Marca productos restantes como 'active'
- Muestra estad铆sticas antes/despu茅s

**Resultados de la migraci贸n:**
```
Total products: 79,997
Active: 77,596
404: 2,401
```

### 3. Modos de Operaci贸n del Scraper
**Archivo modificado:** `scraper.js`

**Nuevos modos:**

#### Modo Daily (`--mode=daily`)
- Scrapea solo productos con `status != '404'`
- L铆mite: 5,000 productos/d铆a
- Ordena por `last_scraped ASC` (prioriza m谩s antiguos)
- **Uso:** Actualizaci贸n diaria r谩pida de precios

#### Modo Weekly (`--mode=weekly`)
- Descarga todos los sitemaps (~76,000 URLs)
- Obtiene URLs marcadas como 404 de la DB
- **MERGE INTELIGENTE:** Usa Set para combinar sin duplicados
- Scrapea todo en una sola pasada
- **Logging especial:**
  ```
  URLs from sitemap: 76809
  URLs from 404s: 2401
  Total unique URLs after merge: 79210
  Duplicates removed: 0
  ```
- **Uso:** Descubrimiento semanal + recuperaci贸n de 404s

#### Modo Legacy (`--from-db`)
- Mantiene compatibilidad con versi贸n anterior
- Scrapea todas las URLs de la DB sin filtro

### 4. Nuevas Funciones
**Archivo:** `scraper.js`

**`fetchUrlsFromDatabase(db, mode)`**
- Soporta modos 'daily' y 'from-db'
- Aplica filtros seg煤n el modo
- Retorna URLs listas para scrapear

**`fetch404Urls(db)`**
- Obtiene URLs con status='404' de `products`
- Obtiene URLs con status_code=404 de `scraping_failures`
- Merge y deduplicaci贸n autom谩tica
- Logging detallado

### 5. Actualizaci贸n de Estado
**Archivo:** `scraper.js` (funci贸n principal)

**Productos exitosos:**
- Se marcan como `status='active'`
- Se actualiza `last_check` y `last_scraped`
- Se elimina registro de `scraping_failures` si exist铆a

**Productos con error 404:**
- Se marca `status='404'` en tabla products
- Se actualiza `last_check`
- Se registra en `scraping_failures`

### 6. GitHub Actions Workflows

#### Workflow Diario
**Archivo nuevo:** `.github/workflows/daily-scraper.yml`

- **Schedule:** Lunes-S谩bado a las 2 AM UTC
- **Cron:** `0 2 * * 1-6`
- **Comando:** `node scraper.js --mode=daily`
- **Duraci贸n estimada:** ~15 min/d铆a

#### Workflow Semanal
**Archivo nuevo:** `.github/workflows/weekly-discovery.yml`

- **Schedule:** Domingos a las 3 AM UTC
- **Cron:** `0 3 * * 0`
- **Comando:** `node scraper.js --mode=weekly`
- **Duraci贸n estimada:** ~50 min/semana

#### Workflow Eliminado
**Archivo eliminado:** `.github/workflows/scrape.yml`
- Reemplazado por los workflows daily y weekly

### 7. Documentaci贸n
**Archivo modificado:** `README.md`

**Secciones agregadas/actualizadas:**
- Explicaci贸n de los dos workflows (daily y weekly)
- Ventajas de la arquitectura
- Modos de operaci贸n del scraper
- Instrucciones de migraci贸n de DB
- Schema actualizado con nuevas columnas
- Estimaciones de uso de minutos

##  Beneficios

### Eficiencia de Minutos
**Antes:**
- Scrapeo diario completo: ~40 min/d铆a  30 = 1200 min/mes

**Ahora:**
- Daily (6 d铆as/semana): ~15 min  24 = 360 min/mes
- Weekly (1 d铆a/semana): ~50 min  4 = 200 min/mes
- **Total: ~560 min/mes (28% del l铆mite)**
- **Ahorro: 640 min/mes (53% de reducci贸n)**

### Optimizaci贸n de Requests
- **No se recorre 2 veces:** Sitemap + 404s se procesan en una sola pasada
- **Deduplicaci贸n autom谩tica:** Si un 404 volvi贸 al sitemap, solo se scrapea 1 vez
- **Priorizaci贸n inteligente:** Daily solo actualiza productos activos

### Recuperaci贸n de 404s
- Productos marcados como 404 se revisan semanalmente
- Si vuelven a estar disponibles, se detectan autom谩ticamente
- Estado se actualiza a 'active' al scrapear exitosamente

##  Uso

### Para ejecutar localmente:

**Migraci贸n (solo una vez):**
```bash
node migrate_db.js
```

**Daily scrape:**
```bash
node scraper.js --mode=daily
```

**Weekly scrape:**
```bash
node scraper.js --mode=weekly
```

### En GitHub Actions:
- Los workflows se ejecutan autom谩ticamente seg煤n el schedule
- Se pueden triggear manualmente desde la pesta帽a "Actions"

##  Notas Importantes

1. **Migraci贸n requerida:** Ejecutar `migrate_db.js` antes de usar los nuevos modos
2. **Compatibilidad:** El modo `--from-db` mantiene comportamiento anterior
3. **Logging mejorado:** Weekly mode muestra estad铆sticas del merge
4. **Estado autom谩tico:** Los productos se marcan como 404 o active autom谩ticamente
