# Unimart Price Tracker üè™üìä

A lightweight price tracking application for unimart.com products, similar to CamelCamelCamel. This tool automatically monitors product prices daily and displays historical trends in your browser - no backend or authentication required!

## üöÄ C√≥mo Ver la Aplicaci√≥n

### ‚ö†Ô∏è IMPORTANTE: Haz el Repositorio P√∫blico Primero

Para usar las opciones online (GitHub Pages, Netlify), necesitas hacer el repositorio p√∫blico:

1. Ve a tu repositorio en GitHub: `https://github.com/andreileonsalas/unimartMonitor`
2. Click en **Settings** (‚öôÔ∏è)
3. Baja hasta el final de la p√°gina
4. En la secci√≥n "Danger Zone", click en **Change visibility**
5. Selecciona **Make public**
6. Confirma escribiendo el nombre del repositorio

**¬øPor qu√© hacerlo p√∫blico?**
- GitHub Pages gratis solo funciona con repos p√∫blicos
- Los datos ya son precios p√∫blicos de Unimart, no hay informaci√≥n sensible
- Te permite compartir la aplicaci√≥n con otros sin necesidad de dar acceso al repo

---

### üåê Opci√≥n 1: GitHub Pages (RECOMENDADO - Sin instalar nada)

**‚ö†Ô∏è Primero: Haz merge del Pull Request a la rama `main`**

1. Ve a: `https://github.com/andreileonsalas/unimartMonitor/pulls`
2. Busca el Pull Request "Fix scraper reference error" (o similar)
3. Click en **"Merge pull request"**
4. Click en **"Confirm merge"**
5. Espera 30 segundos a que se complete el merge

**Luego, activa GitHub Pages:**

1. Ve a tu repositorio: `https://github.com/andreileonsalas/unimartMonitor`
2. Click en **Settings** (‚öôÔ∏è)
3. En el men√∫ izquierdo, click en **Pages**
4. En "Build and deployment":
   - **Source**: Deploy from a branch
   - **Branch**: main
   - **Folder**: / (root)
5. Click en **Save**
6. Espera 1-2 minutos

**Tu aplicaci√≥n estar√° disponible en:**
```
https://andreileonsalas.github.io/unimartMonitor/
```

‚úÖ **Ventajas:**
- Acceso desde cualquier dispositivo con internet
- Se actualiza autom√°ticamente cuando el GitHub Action actualiza los datos
- No necesitas instalar nada en tu computadora
- Es 100% GRATIS (no requiere pago)

### üåç Opci√≥n 2: Usando CDN (Funciona INMEDIATAMENTE despu√©s del merge)

**‚ö†Ô∏è Nota:** Estas URLs funcionar√°n autom√°ticamente despu√©s de que hagas merge del PR a `main` (ver Opci√≥n 1).

Estas URLs funcionan sin necesidad de activar nada (solo copia y pega en tu navegador):

**Opci√≥n A - jsDelivr CDN (RECOMENDADO):**
```
https://cdn.jsdelivr.net/gh/andreileonsalas/unimartMonitor@main/index.html
```

**Opci√≥n B - RawCDN GitHack:**
```
https://rawcdn.githack.com/andreileonsalas/unimartMonitor/main/index.html
```

‚úÖ **Ventajas:**
- Funciona INMEDIATAMENTE (no necesitas configurar GitHub Pages)
- Solo copia y pega la URL en tu navegador
- Se actualiza autom√°ticamente con cada commit a `main`
- Gratis y sin configuraci√≥n
- S√∫per r√°pido (usa CDN global)

‚ö†Ô∏è **Nota:** Estos servicios usan cach√©, as√≠ que los datos pueden tardar unos minutos en actualizarse despu√©s de que el GitHub Action corra.

### üåç Opci√≥n 3: Netlify Drop (Arrastra y suelta - MUY F√ÅCIL)

**‚úÖ Funciona con repo p√∫blico o privado**

1. Ve a [https://app.netlify.com/drop](https://app.netlify.com/drop)
2. Descarga estos 3 archivos de tu repositorio:
   - `index.html`
   - `viewer.js`
   - `prices.db`
3. Arrastra los 3 archivos a la zona de Netlify Drop
4. ¬°Listo! Te dar√° una URL como: `https://random-name-123.netlify.app`

‚úÖ **Ventajas:**
- Sin cuenta necesaria (modo an√≥nimo)
- S√∫per r√°pido (arrastra y suelta)
- Gratis
- Funciona aunque el repo sea privado

‚ö†Ô∏è **Nota:** Para actualizar los datos, necesitas volver a subir el archivo `prices.db` actualizado.

### üì± Opci√≥n 4: Directamente desde tu Navegador (Solo para probar)

1. **Descarga solo estos 3 archivos del repositorio:**
   - `index.html`
   - `viewer.js`
   - `prices.db`

2. **Abre `index.html` con tu navegador** (doble clic en el archivo)

‚ö†Ô∏è **Nota**: Algunos navegadores pueden bloquear la carga del archivo SQLite por seguridad. Si ves un error, usa una de las opciones con servidor.

### üíª Opci√≥n 5: Con un Servidor Local (Para desarrollo)

Descarga el repositorio completo y usa cualquiera de estos m√©todos:

**A) Con Python (lo m√°s simple - viene instalado en Mac/Linux):**
```bash
cd unimartMonitor
python -m http.server 8000
```
Luego abre en tu navegador: **http://localhost:8000**

**B) Con Visual Studio Code (s√∫per f√°cil):**
1. Abre la carpeta `unimartMonitor` en VS Code
2. Instala la extensi√≥n "Live Server" (Ritwick Dey)
3. Clic derecho en `index.html` ‚Üí "Open with Live Server"

**C) Con Node.js:**
```bash
npx http-server -p 8000
```
Luego abre: **http://localhost:8000**

### üåê Opci√≥n 6: En tu Servidor Web / Hosting

Si tienes un hosting con cPanel o FTP:

1. Sube estos 3 archivos a tu carpeta web (`public_html`, `www`, etc.):
   - `index.html`
   - `viewer.js`  
   - `prices.db`

2. Accede desde tu navegador: `http://tudominio.com/index.html`

3. Para actualizar precios: descarga el nuevo `prices.db` del repo y s√∫belo (cada vez que el GitHub Action lo actualice)

## üì∏ Preview

![Unimart Price Tracker](https://github.com/user-attachments/assets/356c3b3a-a560-4088-9c20-be243f8eff19)

La aplicaci√≥n muestra:
- üìä Estad√≠sticas totales (productos, registros, √∫ltima actualizaci√≥n)
- üîç Buscador en tiempo real
- üí∞ Precios actuales con moneda (CRC - Colones)
- üìà Historial de cambios de precio al hacer clic en cada producto
- üé® Interfaz moderna y responsiva

## Features

- üìà **Comprehensive Price Tracking**: Automatically scrapes prices from ALL ~38,000 products on unimart.com
- üîÑ **Incremental Scraping**: Processes products progressively over ~25 days to build complete database
- üíæ **SQLite Storage**: All data stored in a single SQLite file - no external database needed
- üåê **Browser-Based Viewer**: View price history directly in your browser using client-side SQLite
- ü§ñ **Automated**: Runs daily via GitHub Actions - no server required
- üîç **Search & Filter**: Easily search through tracked products
- üìâ **Price History**: See how prices change over time for each product
- üé® **Beautiful UI**: Clean, modern interface with price change indicators
- ‚ôªÔ∏è **Smart Deduplication**: Automatically handles products appearing in multiple sitemaps

## How It Works

1. **GitHub Actions** runs daily to:
   - Fetch the sitemap index from https://www.unimart.com/sitemap.xml (1,228 product sitemaps)
   - Process 100 sitemaps per run (~2,400 products)
   - Scrape 50 products per run with rate limiting
   - Use Set-based deduplication to avoid duplicate entries
   - Track progress in database to resume where it left off
   - Store data in SQLite database (`prices.db`)
   - Commit changes back to the repository
   - **Complete cycle**: ~25 days to scrape all products, then starts over to update prices

2. **Browser Viewer** (`index.html`):
   - Loads the SQLite database directly in your browser using sql.js
   - Displays all tracked products with current prices
   - Shows price trends and history
   - Fully client-side - no backend needed!

## Setup

### Prerequisites

- Node.js 20 or higher (requerido para el scraper)
- npm

### Installation

1. Clone this repository:
```bash
git clone https://github.com/andreileonsalas/unimartMonitor.git
cd unimartMonitor
```

2. Install dependencies:
```bash
npm install
```

### Running the Scraper

To run the scraper manually:

```bash
npm run scrape
```

This will:
- Fetch the sitemap from unimart.com
- Extract product URLs
- Scrape prices from up to 50 products
- Save data to `prices.db`

## ü§ñ Automated Daily Tracking

The GitHub Actions workflow (`.github/workflows/scrape.yml`) runs automatically every day at 2 AM UTC. It will:

1. Install dependencies
2. Run the scraper
3. Commit the updated database
4. Push changes to the repository

**Trigger manual:** Puedes ejecutarlo manualmente desde GitHub:
- Ve a "Actions" tab en GitHub
- Selecciona "Daily Price Scraper"
- Clic en "Run workflow"

---

## üîß Para Desarrolladores

### Prerequisites

- Node.js 20 or higher (requerido para el scraper)
- npm

### Installation

1. Clone this repository:
```bash
git clone https://github.com/andreileonsalas/unimartMonitor.git
cd unimartMonitor
```

2. Install dependencies:
```bash
npm install
```

### Running the Scraper Locally

To run the scraper manually:

```bash
npm run scrape
```

This will:
- Fetch the sitemap from unimart.com
- Extract product URLs
- Scrape prices from up to 50 products
- Save data to `prices.db`

The GitHub Actions workflow (`.github/workflows/scrape.yml`) runs automatically every day at 2 AM UTC. It will:

1. Install dependencies
2. Run the scraper
3. Commit the updated database
4. Push changes to the repository

You can also trigger it manually:
- Go to "Actions" tab in GitHub
- Select "Daily Price Scraper"
- Click "Run workflow"

## Database Schema

### Products Table
- `id`: Primary key
- `url`: Product URL (unique identifier)
- `sku`: Product SKU from Unimart (indexed for fast search)
- `title`: Product title
- `last_scraped`: Last scrape timestamp

**Note:** Both URL and SKU are stored to maintain dual reference. If Unimart changes the URL or SKU, historical data is preserved. The URL column has a UNIQUE constraint to prevent duplicate entries.

### Prices Table
- `id`: Primary key
- `product_id`: Foreign key to products
- `price`: Product price
- `currency`: Currency code (CRC for Costa Rican Col√≥n, USD, EUR, etc.)
- `scraped_at`: Timestamp of scrape

### Scraping State Table (New)
- `id`: Primary key (always 1)
- `last_sitemap_index`: Index of the last processed sitemap (0-1227)
- `total_sitemaps`: Total number of sitemaps (1228)
- `last_updated`: Last update timestamp

This table tracks scraping progress across runs, enabling the scraper to resume where it left off.

## Configuration

### Scraper Settings

The scraper now uses **incremental scraping** to process ALL products from Unimart over time:

Edit `scraper.js` to customize:

- **Products per run**: `const MAX_PRODUCTS_PER_RUN = 50;` - Number of products to scrape in each run
- **Sitemaps per run**: `const MAX_SITEMAPS_PER_RUN = 100;` - Number of sitemaps to fetch in each run  
- **Request delay**: `const REQUEST_DELAY_MS = 1000;` - Delay between product scrapes (milliseconds)
- **Sitemap delay**: `const SITEMAP_DELAY_MS = 500;` - Delay between sitemap fetches (milliseconds)

#### How Incremental Scraping Works

1. **Sitemap Processing**: There are 1,228 product sitemaps with ~38,000 total products
2. **Batch Processing**: Each run processes 100 sitemaps (~2,400 products)
3. **State Tracking**: Progress is saved in the database's `scraping_state` table
4. **Automatic Cycling**: After processing all 1,228 sitemaps, it starts over from the beginning
5. **Smart Prioritization**: New products are scraped first, then existing products are updated
6. **Deduplication**: Products appearing in multiple sitemaps are automatically deduplicated by URL

**Timeline**: With default settings (100 sitemaps/run, 50 products/run):
- Complete first pass: ~25 daily runs (~25 days)
- All 38,000+ products will be tracked
- Price history builds up over time for trend analysis

**Benefits**:
- ‚úÖ Processes ALL products from the sitemap (not just 50)
- ‚úÖ Respectful to Unimart's servers (rate limited)
- ‚úÖ Builds comprehensive price history over time
- ‚úÖ Automatic duplicate handling
- ‚úÖ Resumes where it left off after each run

### Workflow Schedule

Edit `.github/workflows/scrape.yml` to change the schedule:

```yaml
schedule:
  - cron: '0 2 * * *'  # Runs daily at 2 AM UTC
```

## Technologies Used

- **Node.js**: Scraper runtime
- **Axios**: HTTP requests
- **Cheerio**: HTML parsing
- **xml2js**: Sitemap parsing
- **better-sqlite3**: SQLite database for Node.js
- **sql.js**: SQLite in the browser (WebAssembly)
- **GitHub Actions**: Automation
- **Vanilla JavaScript**: Browser viewer (no frameworks)

## Project Structure

```
unimartMonitor/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ scrape.yml          # GitHub Actions workflow
‚îú‚îÄ‚îÄ scraper.js                  # Main scraper script
‚îú‚îÄ‚îÄ index.html                  # Browser viewer UI
‚îú‚îÄ‚îÄ viewer.js                   # Browser viewer logic
‚îú‚îÄ‚îÄ prices.db                   # SQLite database (generated)
‚îú‚îÄ‚îÄ package.json                # Node.js dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üîß Si Unimart Cambia su Estructura

### Edge Cases Verificados

‚úÖ **Sitemap Index**: El sitemap principal tiene 1,245 sitemaps referenciados
‚úÖ **Product Sitemaps**: Los primeros 1,228 contienen productos
‚úÖ **Otros Sitemaps**: Los √∫ltimos son collections/articles/blogs (NO productos)
‚úÖ **Estructura Uniforme**: Todos los product sitemaps tienen el mismo formato
‚úÖ **Uso del Primero**: Es SEGURO usar solo el primer sitemap - no hay diferencias

### D√≥nde Hacer Cambios

#### 1. Si Cambia la URL del Sitemap

**Archivo**: `scraper.js` (l√≠nea 10)

```javascript
// üîß CAMBIAR AQU√ç si la URL del sitemap cambia
const SITEMAP_URL = 'https://www.unimart.com/sitemap.xml';
```

## TODO (para una IA que edite o mantenga el proyecto)

Breve checklist y contexto para que una IA (o nuevo desarrollador) entienda r√°pidamente qu√© hacer y pueda editar el scraper de forma segura:

- **Contexto del proyecto**: `scraper.js` descarga el `sitemap.xml`, extrae sitemaps de productos y scrapea p√°ginas de productos, guardando resultados en `prices.db` (SQLite) usando `better-sqlite3`.
- **Dependencias clave**: `axios`, `cheerio`, `xml2js`, `better-sqlite3`.
- **D√≥nde comenzar**: revisar `scraper.js` ‚Äî funciones importantes: `initDatabase()`, `fetchSitemap(db)`, `scrapeProduct(url)`, `saveProductPrice(db, productData)`, `main()`.

- **Qu√© necesita la IA para entender/editar correctamente**:
   - Conocer los selectores actuales de la p√°gina de producto (`h1`, `.money`, script JSON con `sku`) y c√≥mo parsear precios/moneda.
   - Entender el esquema de la base de datos (tablas `products`, `prices`, `scraping_state`, `scraping_failures`, `sitemap_cache`).
   - Saber que `MAX_SITEMAPS_PER_RUN = 0` significa procesar todos los sitemaps en una sola ejecuci√≥n.
   - Respetar los delays (`REQUEST_DELAY_MS`, `SITEMAP_DELAY_MS`) y los l√≠mites de concurrencia (`PARALLEL_REQUESTS`, `SITEMAP_PARALLEL_REQUESTS`) para evitar bloqueos.

- **Acciones seguras que puede automatizar la IA**:
   - Actualizar selectores de `scrapeProduct()` si la estructura HTML cambia.
   - Ajustar `PARALLEL_REQUESTS` y `REQUEST_DELAY_MS` si se observan bloqueos o lentitud.
   - A√±adir reintentos/backoff en descargas fallidas (sitemaps y productos).
   - A√±adir pruebas unitarias para parsing de precio/sku a partir de muestras HTML.

- **Acciones que requieren revisi√≥n humana**:
   - Cambios que impliquen almacenar nuevas columnas en `prices.db` o migraciones de esquema.
   - Pol√≠ticas de eliminaci√≥n de productos (por ejemplo, marcar productos como 'retirados' tras N errores 404).
   - Cambios que aumenten significativamente la carga sobre `unimart.com` (aumentar paralelismo sin pruebas).

- **Comandos √∫tiles para debugging y ver el estado**:
   - `node find_product.js <product_url>` ‚Üí busca un producto en la DB.
   - `node show_failure.js <product_url>` ‚Üí muestra registros en `scraping_failures`.
   - `node reset_scraping_state.js` ‚Üí reinicia `last_sitemap_index` a 0.
   - `node download_sitemaps.js` ‚Üí descarga copias locales de los sitemaps (√∫til para pruebas offline).

- **D√≥nde buscar logs**:
   - `error.log` ‚Üí errores acumulados (se imprimen al final de cada ejecuci√≥n para CI).
   - Salida est√°ndar (stdout) ‚Üí progreso y `Cache HIT`/`Cache MISS` logs.

- **Prueba recomendada despu√©s de cambiar el scraper**:
   1. Ejecutar `node scraper.js` en modo corto (`MAX_SITEMAPS_PER_RUN = 5`) para validar parsing y logs.
   2. Revisar `error.log` y la tabla `scraping_failures` por fallos.
   3. Revisar `prices.db` con `find_product.js`.

Esta secci√≥n est√° pensada para que una IA pueda tener un checklist claro y act√∫e con cautela sobre los cambios que impactan datos o tr√°fico.

#### 2. Si Cambia la Estructura del Sitemap Index

**Archivo**: `scraper.js` (l√≠neas 58-90)

```javascript
// üîß CAMBIAR AQU√ç si la estructura del sitemap index cambia
if (result.sitemapindex && result.sitemapindex.sitemap) {
  // Ajusta c√≥mo se extraen las referencias a otros sitemaps
  const firstSitemapUrl = result.sitemapindex.sitemap[0].loc[0];
  // ...
}
```

#### 3. Si Cambia el Selector de Precio en la P√°gina

**Archivo**: `scraper.js` (l√≠nea 165)

```javascript
// üîß CAMBIAR AQU√ç si el HTML de la p√°gina de producto cambia
// Actualmente el precio est√° en la clase .money
const priceText = $('.money').first().text().trim() ||
                $('.price').first().text() ||
                // Agrega nuevos selectores aqu√≠
```

**C√≥mo verificar el nuevo selector:**
1. Abre una p√°gina de producto en unimart.com
2. Click derecho ‚Üí "Inspeccionar elemento" en el precio
3. Encuentra la clase o ID del elemento
4. Actualiza el selector en el c√≥digo

#### 4. Si Cambia el Formato del SKU

**Archivo**: `scraper.js` (l√≠nea 156)

```javascript
// üîß CAMBIAR AQU√ç si el formato del SKU en el JSON cambia
const skuMatch = content.match(/"sku"\s*:\s*"([^"]+)"/);
```

#### 5. Si Cambia el S√≠mbolo de Moneda

**Archivo**: `scraper.js` (l√≠neas 177-185)

```javascript
// üîß CAMBIAR AQU√ç si cambian de s√≠mbolo de moneda
if (priceText.includes('‚Ç°')) {
  currency = 'CRC';
} // Agrega nuevos s√≠mbolos aqu√≠
```

### Herramientas para Debugging

```bash
# Ver la estructura del sitemap actual
curl https://www.unimart.com/sitemap.xml | head -100

# Ver el HTML de una p√°gina de producto
curl https://www.unimart.com/products/[nombre-producto] | grep -i "price\|money"

# Probar el scraper manualmente
npm run scrape
```

## Troubleshooting

### GitHub Pages or CDN URLs showing empty

If https://andreileonsalas.github.io/unimartMonitor/ or the CDN URLs show "0 products":

1. **Verify files are on main branch**: The files must be committed to the `main` branch for deployment
2. **Check database exists**: Ensure `prices.db` is committed and pushed to `main`
3. **Merge this PR**: This PR contains the fixes - merge it to `main` first
4. **Wait for deployment**: GitHub Pages can take 2-5 minutes to deploy after merging
5. **Hard refresh**: Press Ctrl+F5 (Windows/Linux) or Cmd+Shift+R (Mac) to clear cache
6. **Check browser console**: Open DevTools (F12) and look for any JavaScript errors
7. **Verify SQL.js loads**: The page needs to load SQL.js from CDN - check Network tab in DevTools

**Common issues**:
- **Database not on main**: Run the scraper and ensure the updated `prices.db` is on the `main` branch
- **CDN blocking**: Some ad blockers or privacy extensions may block CDN resources
- **Cache**: Browser cache may be showing old version - do a hard refresh
- **GitHub Pages not enabled**: Verify GitHub Pages is enabled in repository Settings ‚Üí Pages

### Database not loading in browser
- Make sure you have run the scraper at least once
- Verify `prices.db` exists in the repository on the `main` branch  
- Check browser console for errors (F12 ‚Üí Console tab)
- Try accessing the database directly: `https://andreileonsalas.github.io/unimartMonitor/prices.db`

### Scraper not finding products
- The product URL patterns may need adjustment
- Check if unimart.com's sitemap structure has changed
- Modify the URL filter logic in `scraper.js`
- Check the scraping_state table: `sqlite3 prices.db "SELECT * FROM scraping_state;"`

### Price extraction issues
- Product page HTML selectors may need updating
- Check the actual HTML structure of unimart.com product pages
- Adjust the selectors in the `scrapeProduct()` function

### Incremental scraping progress

To check scraping progress:

```bash
# Check how many products and prices are in the database
sqlite3 prices.db "SELECT COUNT(*) FROM products; SELECT COUNT(*) FROM prices;"

# Check scraping state (which sitemap we're on)
sqlite3 prices.db "SELECT * FROM scraping_state;"

# View recently added products
sqlite3 prices.db "SELECT title, price, currency FROM products p JOIN prices pr ON p.id = pr.product_id ORDER BY pr.scraped_at DESC LIMIT 10;"
```

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
